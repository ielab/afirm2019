{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing Cluweb12B (used in TREC Web Track 2013-2014, CLEF2016 and CLEF2017)\n",
    "This repository describes how to index the Clueweb 12B corpus (short version) using Elasticsearch 5.x.x.\n",
    "The index will have two fields: title and body.\n",
    "Each field will have its own custom simmilarity definition to enable similarity tuning.\n",
    "\n",
    "Pre-processing applied to both fields including:\n",
    "* lowercasing\n",
    "* removing stop words based on terrier stop words list\n",
    "* steeming using porter stemer\n",
    "\n",
    "## Pre-requisite\n",
    "* Elasticsearch 5.x.x\n",
    "* Python 2.7\n",
    "* Elasticsearch python api. can be found from [here](https://elasticsearch-py.readthedocs.io/en/master/)\n",
    "* Clueweb12B corpus files\n",
    "\n",
    "## Preparing the Elasticsearch\n",
    "Since the terrier stop words list is not included in Elasticsearch 5.1.1,\n",
    "we need need to make sure that the terrier-stop.txt file is available in /config/stopwords folder within the ElasticSearch folder.\n",
    "\n",
    "\n",
    "## Indexing using Python\n",
    "Lets move to python. First, we need to import the necessary libaries:\n",
    "* gzip: to unzip Aquaint collection files\n",
    "* warc: clueweb12 corpus is in warc file\n",
    "* time: to measure running time\n",
    "* glob: to traverse all Aquaint collection files\n",
    "* re: regular experession to search unwanted phrase (e.g., special characters)\n",
    "* sys: to measure size of bulk package variable\n",
    "* elasticsearch: to work with Elasticsearch\n",
    "* lxml.html: clueweb12 warc files contains HTMLs, so we need this library to parse the HTML\n",
    "* multiprocessing: clueweb12 is a very big corpus, to make it faster, we use multiprocessing to index in parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import warc\n",
    "import time\n",
    "import glob\n",
    "import lxml.html\n",
    "import re\n",
    "import io\n",
    "import sys\n",
    "from elasticsearch import Elasticsearch\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify location of the Clueweb12B files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warcPath = \"~\\Clueweb12B_sample\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open connection to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es0 = Elasticsearch(urls='http://localhost', port=9200, timeout=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify bulk size and max documents in a bulk. For faster indexing, we will index documents in bulks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_size = 4000\n",
    "bulk_count = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define index name and document type for the aquaint index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexName = \"clueweb12b_all\"\n",
    "docType = \"clueweb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the index settings and mappings. Variable definition:\n",
    "* \"number_of_shards\": 1 --> all documents will be indexed in one shard (i.e. no partitions)\n",
    "* \"number_of_replicas\": 0 --> no replica (i.e. no back up index)\n",
    "* \"my_english\" --> custom analyzer telling the ES to use \"standard\" english tokenizer,\n",
    "lowercase all characters, remove stop words based on custom \"terrier_stopwords\",\n",
    "stem using porter stemer.\n",
    "* \"terrier_stopwords\" --> custom stopwords definition based on the stopwords/terrier-stop.txt file\n",
    "* \"similarity\" --> custom similarity object for each field\n",
    "* \"_source\" --> specify to store or not to store the document text in the index (False means not storing the document text)\n",
    "* \"properties\" --> field definition. utilize the defined custom similarity and analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_body = {\n",
    "    \"settings\": {\n",
    "      \"number_of_shards\": 1,\n",
    "      \"number_of_replicas\": 0,\n",
    "      \"analysis\": {\n",
    "        \"analyzer\": {\n",
    "            \"my_english\": {\n",
    "                \"tokenizer\": \"standard\",\n",
    "                \"filter\": [\"lowercase\", \"terrier_stopwords\", \"porter_stem\"]\n",
    "            }\n",
    "        },\n",
    "        \"filter\": {\n",
    "          \"terrier_stopwords\": {\n",
    "              \"type\": \"stop\",\n",
    "              \"stopwords_path\": \"stopwords/terrier-stop.txt\"\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"similarity\": {\n",
    "        \"sim_title\": {\n",
    "            \"type\": \"BM25\",\n",
    "            \"b\": 0.75,\n",
    "            \"k1\": 1.2\n",
    "        },\n",
    "        \"sim_body\": {\n",
    "            \"type\": \"BM25\",\n",
    "            \"b\": 0.75,\n",
    "            \"k1\": 1.2\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "      docType: {\n",
    "        \"_source\": {\n",
    "            \"enabled\": False\n",
    "        },\n",
    "        \"properties\": {\n",
    "            \"title\": {\n",
    "                 \"type\": \"text\",\n",
    "                 \"similarity\": \"sim_title\",\n",
    "                 \"analyzer\": \"my_english\"\n",
    "            },\n",
    "            \"body\": {\n",
    "                \"type\": \"text\",\n",
    "                \"similarity\": \"sim_body\",\n",
    "                \"analyzer\": \"my_english\"\n",
    "            }\n",
    "         }\n",
    "      }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create index based on the specified settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not es0.indices.exists(indexName):\n",
    "    print (\"creating \", indexName, \" index, start at \", startTime)\n",
    "    res = es0.indices.create(index=indexName, body=request_body)\n",
    "    print(\" response: '%s'\" % res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create indexing function which will be executed in parallel.\n",
    "This function accept path to a single gziped warc file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_index(fname):\n",
    "    i = 0\n",
    "    totalSize = 0\n",
    "    bulk_data = []\n",
    "    lapTime = time.time()\n",
    "    es = Elasticsearch(urls='http://localhost', port=9200, timeout=600)\n",
    "\n",
    "    print(\"Processing file: {}\".format(fname))\n",
    "    with gzip.open(fname, mode='rb') as gzf:\n",
    "        WarcTotalDocuments = 0\n",
    "        EmptyDocuments = 0\n",
    "        for record in warc.WARCFile(fileobj=gzf):\n",
    "            if record.header.get('WARC-Type').lower() == 'warcinfo':\n",
    "                WarcTotalDocuments = record.header.get('WARC-Number-Of-Documents')\n",
    "\n",
    "            if record.header.get('WARC-Type').lower() == 'response':\n",
    "                docId = record.header.get('WARC-Trec-ID')\n",
    "                docString = record.payload.read()\n",
    "\n",
    "                htmlStart = docString.find('<html')\n",
    "                if htmlStart < 1:\n",
    "                    htmlStart = docString.find('<HTML')\n",
    "                if htmlStart < 1:\n",
    "                    htmlStart = docString.find('<Html')\n",
    "\n",
    "                if htmlStart < 1:\n",
    "                    EmptyDocuments += 1\n",
    "                else:\n",
    "                    # extract and scrub html string\n",
    "                    htmlString = docString[htmlStart:]\n",
    "                    htmlString = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\xff]', '', htmlString)\n",
    "                    htmlString = re.sub(r'&\\w{4,6};', '', htmlString)\n",
    "                    htmlString = htmlString.replace(\",\", \" \").replace(\"-\", \" \").replace(\".\", \" \")\n",
    "\n",
    "                    fContent = io.BytesIO(str(htmlString.decode(\"utf-8\", \"ignore\")))\n",
    "\n",
    "                    try:\n",
    "                        htmlDoc = lxml.html.parse(fContent)\n",
    "\n",
    "                        # the html.xpath return an array so need to convert it to string with join method\n",
    "                        title = \" \".join(htmlDoc.xpath('//title/text()'))\n",
    "\n",
    "                        rootClean = htmlDoc.getroot()\n",
    "\n",
    "                        body = \" - \"\n",
    "                        try:\n",
    "                            body = rootClean.body.text_content()\n",
    "                            body = ' '.join(word for word in body.split() if word.isalnum())\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        content = title + body\n",
    "                        bulk_meta = {\n",
    "                            \"index\": {\n",
    "                                \"_index\": indexName,\n",
    "                                \"_type\": docType,\n",
    "                                \"_id\": docId\n",
    "                            }\n",
    "                        }\n",
    "\n",
    "                        bulk_content = {\n",
    "                            'title': title,\n",
    "                            'body': body\n",
    "                        }\n",
    "\n",
    "                        bulk_data.append(bulk_meta)\n",
    "                        bulk_data.append(bulk_content)\n",
    "                        totalSize += (sys.getsizeof(content) / 1024)  # convert from bytes to KiloBytes\n",
    "\n",
    "                        i += 1\n",
    "                        if totalSize > bulk_size or i % bulk_count == 0:\n",
    "                            res = es.bulk(index=indexName, doc_type=docType, body=bulk_data, refresh=False)\n",
    "                            bulk_data = []\n",
    "                            totalSize = 0\n",
    "                    except:\n",
    "                        print(\"Error processing document: {}\".format(docId))\n",
    "                        raise\n",
    "\n",
    "        if len(bulk_data) > 0:\n",
    "            # index the remainder files\n",
    "            res = es.bulk(index=indexName, doc_type=docType, body=bulk_data, refresh=False)\n",
    "\n",
    "        print(\"File {0} Completed, Duration: {1} sec, Total: {2}, Processed: {3}, Empty: {4}, Variance: {5}\".\n",
    "               format(fname, time.time() - lapTime, WarcTotalDocuments, str(i), str(EmptyDocuments),\n",
    "                      str(int(WarcTotalDocuments) - i - EmptyDocuments)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traverse all folder and parallely process all gzipped warc files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warcFolder = glob.glob(warcPath + \"*\")\n",
    "for warcFold in warcFolder:\n",
    "    folders = glob.glob(warcFold + \"/*\")\n",
    "    print(\"Processing Path: {}\".format(warcFold))\n",
    "\n",
    "    for fold in folders:\n",
    "        print(\"Processing folder: {}\".format(fold))\n",
    "        p = multiprocessing.Pool()\n",
    "        resultString = p.map(es_index, glob.glob(fold + \"/*\"))\n",
    "        p.close()\n",
    "        p.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
